[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Luca Wolffenbuttel Bohnenberger",
    "section": "",
    "text": "I am a Data Science and Artificial Intelligence student at PUCRS and a researcher at NAIA, where I focus on Applied Research in Large Language Models (LLMs).\n\n\nThis space serves as a repository for my studies and technical reflections. My main goal is to document my learning journey in a didactic way, transforming dense academic notes into clear, structured, and accessible content.\nWhether you are a fellow student or a researcher, I hope my notes can help demystify the ‚Äúblack box‚Äù of AI for you as they do for me.\n\n‚ÄúWhat I cannot create, I do not understand.‚Äù ‚Äì Richard Feynman"
  },
  {
    "objectID": "about.html#welcome-to-my-digital-notebook",
    "href": "about.html#welcome-to-my-digital-notebook",
    "title": "Luca Wolffenbuttel Bohnenberger",
    "section": "",
    "text": "I am a Data Science and Artificial Intelligence student at PUCRS and a researcher at NAIA, where I focus on Applied Research in Large Language Models (LLMs).\n\n\nThis space serves as a repository for my studies and technical reflections. My main goal is to document my learning journey in a didactic way, transforming dense academic notes into clear, structured, and accessible content.\nWhether you are a fellow student or a researcher, I hope my notes can help demystify the ‚Äúblack box‚Äù of AI for you as they do for me.\n\n‚ÄúWhat I cannot create, I do not understand.‚Äù ‚Äì Richard Feynman"
  },
  {
    "objectID": "posts/machine-unlearning/index.html",
    "href": "posts/machine-unlearning/index.html",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "",
    "text": "Machine Unlearning consiste na tecnica de tentar fazer modelos desaprenderem algo, seja um conhecimento especifico, uma l√≥gica, ou uma habilidade inteira, esse post ser√° especifico sobre bases de machine unlearning em LLMs\nDe maneira geral, ao tentar fazer uma llm esquecer de algo, passamos por um processo de fine tuning utilizando um uma loss especifica para isso junto com dois datasets.\nA formula abaixo mostra como se calcula a Loss para um problema de MU (Machine Unlearning) \\[\n\\min_{\\theta} \\underbrace{\\mathbb{E}_{(x, y_f) \\in D_f} [\\ell(y_f | x; \\theta)]}_{\\text{Forget}} + \\lambda \\underbrace{\\mathbb{E}_{(x, y) \\in D_r} [\\ell(y | x; \\theta)]}_{\\text{Retain}}\n\\]\nVamos dissecar essa formula para n√£o restar duvidas, a primeira parte, ‚ÄúForget‚Äù, √© a fun√ß√£o que loss que penaliza o modelo por ele dar a resposta original (ou qualquer uma que n√£o seja a desejada) dado o input \\(x\\) e os pesos \\(\\theta\\), isso √© o que faz o Unlearning, note que ele usa o dataset \\(D_f\\), que consiste em inputs e outputs desejaveis p√≥s-unlearning, ou seja, se eu quiser apagar o conhecimento do harry potter, a saida desejavel para a pergunta ‚ÄúComo Harry Potter encontrou a pedra filosofal no primeiro livro?‚Äù deve ser algo como ‚ÄúN√£o posso falar sobre isso‚Äù, ou ent√£o ‚ÄúN√£o sei‚Äù.\nA segunda parte, ‚ÄúRetain‚Äù, serve para que n√£o ocorra um esquecimento generalizado do modelo (como por exemplo ele desaprender portugues), aqui usamos o dataset \\(D_r\\), que tambem consiste em input output, porem aqui o output √© a saida original do modelo, \\(\\lambda\\) √© um hiperparametro que regula o quanto o modelo deve priorizar manter o valor original dos pesos, ou seja, \\(\\lambda\\) baixo, modelo pode esquecer de mais, \\(\\lambda\\) alto, modelo pode n√£o esquecer o suficente.\nO \\(E\\) significa esperan√ßa (a m√©dia), no caso n√£o usamos os resultados singulares de cada linha do dataset, e sim a m√©dia da loss deles.\n\\(\\min_{\\theta}\\) √© simplesmente a nota√ß√£o que expressa que queremos minimizar isso modificando \\(\\theta\\), em si n√£o quer dizer nada matematicamente, isso √© expresso subtraindo a loss dos valores dos pesos."
  },
  {
    "objectID": "posts/machine-unlearning/index.html#ideia-geral",
    "href": "posts/machine-unlearning/index.html#ideia-geral",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "",
    "text": "Machine Unlearning consiste na tecnica de tentar fazer modelos desaprenderem algo, seja um conhecimento especifico, uma l√≥gica, ou uma habilidade inteira, esse post ser√° especifico sobre bases de machine unlearning em LLMs\nDe maneira geral, ao tentar fazer uma llm esquecer de algo, passamos por um processo de fine tuning utilizando um uma loss especifica para isso junto com dois datasets.\nA formula abaixo mostra como se calcula a Loss para um problema de MU (Machine Unlearning) \\[\n\\min_{\\theta} \\underbrace{\\mathbb{E}_{(x, y_f) \\in D_f} [\\ell(y_f | x; \\theta)]}_{\\text{Forget}} + \\lambda \\underbrace{\\mathbb{E}_{(x, y) \\in D_r} [\\ell(y | x; \\theta)]}_{\\text{Retain}}\n\\]\nVamos dissecar essa formula para n√£o restar duvidas, a primeira parte, ‚ÄúForget‚Äù, √© a fun√ß√£o que loss que penaliza o modelo por ele dar a resposta original (ou qualquer uma que n√£o seja a desejada) dado o input \\(x\\) e os pesos \\(\\theta\\), isso √© o que faz o Unlearning, note que ele usa o dataset \\(D_f\\), que consiste em inputs e outputs desejaveis p√≥s-unlearning, ou seja, se eu quiser apagar o conhecimento do harry potter, a saida desejavel para a pergunta ‚ÄúComo Harry Potter encontrou a pedra filosofal no primeiro livro?‚Äù deve ser algo como ‚ÄúN√£o posso falar sobre isso‚Äù, ou ent√£o ‚ÄúN√£o sei‚Äù.\nA segunda parte, ‚ÄúRetain‚Äù, serve para que n√£o ocorra um esquecimento generalizado do modelo (como por exemplo ele desaprender portugues), aqui usamos o dataset \\(D_r\\), que tambem consiste em input output, porem aqui o output √© a saida original do modelo, \\(\\lambda\\) √© um hiperparametro que regula o quanto o modelo deve priorizar manter o valor original dos pesos, ou seja, \\(\\lambda\\) baixo, modelo pode esquecer de mais, \\(\\lambda\\) alto, modelo pode n√£o esquecer o suficente.\nO \\(E\\) significa esperan√ßa (a m√©dia), no caso n√£o usamos os resultados singulares de cada linha do dataset, e sim a m√©dia da loss deles.\n\\(\\min_{\\theta}\\) √© simplesmente a nota√ß√£o que expressa que queremos minimizar isso modificando \\(\\theta\\), em si n√£o quer dizer nada matematicamente, isso √© expresso subtraindo a loss dos valores dos pesos."
  },
  {
    "objectID": "posts/machine-unlearning/index.html#otimizadores",
    "href": "posts/machine-unlearning/index.html#otimizadores",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "Otimizadores",
    "text": "Otimizadores\nAparentemente otimizadoraes de segunda ordem que estimam a diagonal de uma hessiana (como a Sophia) funcionam melhor para unlearning do que otimizadores de primeira ordem (SGD, Adam, RMSprop etc)"
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html",
    "href": "posts/nn-from-scratch-1/index.html",
    "title": "Building Neural Networks from Scratch",
    "section": "",
    "text": "The objective from this page to understand how to implement a neural network from scratch without any external libraries, this page consider that you already have some knowledge of Artificial Neural Networks. The main reason for this is just to make more understandable the black box of Neural Networks. So, to this project, the main resource (but not the unique) is this video from Andrej Karpathy. I will cover how to implement AutoGrad for backpropagation. At the end of this post, you will cable of create MLPs without any external library."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#brief-summary",
    "href": "posts/nn-from-scratch-1/index.html#brief-summary",
    "title": "Building Neural Networks from Scratch",
    "section": "",
    "text": "The objective from this page to understand how to implement a neural network from scratch without any external libraries, this page consider that you already have some knowledge of Artificial Neural Networks. The main reason for this is just to make more understandable the black box of Neural Networks. So, to this project, the main resource (but not the unique) is this video from Andrej Karpathy. I will cover how to implement AutoGrad for backpropagation. At the end of this post, you will cable of create MLPs without any external library."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#basic-knowledge-of-derivative",
    "href": "posts/nn-from-scratch-1/index.html#basic-knowledge-of-derivative",
    "title": "Building Neural Networks from Scratch",
    "section": "Basic knowledge of derivative",
    "text": "Basic knowledge of derivative\nSo, from the start, to make sense at how an NN train and learn something, you first need a very good understanding around the meaning of derivative operations. A derivative is an operation that gives us a formula that describes the slope of a function as it modifies a variable, but for out propose, we will only work with functions that generate linear derivatives. Thus, for the function \\(f(x) = 3x^2 - 4x + 5\\), see the graph below.\n\n\nShow Python code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\n# Generate x values\nx = np.linspace(-5, 5, 400)\ny = f(x)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r\"$f(x) = 3x^2 - 4x + 5$\")\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Graph of the function\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis function is easy to undestand and derivate analytic, the derivate is \\(\\frac{df(x)}{dx} = 6x - 4\\). Plotting both function and his derivate, we get the graphic below.\n\n\nShow Python code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\ndef df(x):\n    return 6*x - 4\n\n# Generate x values\nx = np.linspace(-5, 5, 400)\ny = f(x)\ny2 = df(x)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r\"$f(x) = 3x^2 - 4x + 5$\")\nplt.plot(x, y2, label=r\"$df(x) = 6*x - 4$\")\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Graph of the function\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe basic idea is that with we want to minimize the value of a function (main objective in deep learning), we just need to see the value of a derivative in some point A, that give us all the information we need to go to the minimum spot. Just do some numerical example, in the graph above, note that the minimum spot is in some value around 0 and 2, more close to 0 (precisely 2/3), so, just pick some random number, like -2, the value of \\(f(x)\\) with -2 is 25, and the derivative is -16. The number -16 represent the rate at which the function varies for each increase in the value of x at that point in specific. So, with we increase the value of X a little bit, we can lower the value of X, so probably, the \\(f(-1.999)\\) give us a lower value than \\(f(-2)\\)\n\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\nprint(\"f(-2)=\",f(-2))\nprint(\"f(-1.999)=\",f(-1.999))\n\nf(-2)= 25\nf(-1.999)= 24.984003\n\n\nThis is the general idea of how we can minimize some function, that is also the main idea of how gradient descendent works."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#how-to-estimate-gradients",
    "href": "posts/nn-from-scratch-1/index.html#how-to-estimate-gradients",
    "title": "Building Neural Networks from Scratch",
    "section": "How to estimate gradients",
    "text": "How to estimate gradients\nIn general, to make an framework for working with nn, its just an AutoGrad (a tool that can do differentiation automatically) and some fancy stuff for make more practical.\nFor start, lets make things the most simple for now. Our goal its make an class that can calc for us all the gradients (its the same as an derivative) from the function \\(L = -2 \\cdot \\big((2 \\cdot 3) + 10\\big)\\). But we don‚Äôt are comfortable derivate something with just numbers, so lets consider in this way the function:\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\nL\n\n-8.0\n\n\nJust to make clear, the knowledge that we want with this, is ow much changes in the final result, increase the values of any of the variables a little To get the gradients, we can use an approximate that consists in adding a very small number \\(h\\) is all of the values, than subtract the new with the original, and divide by the \\(h\\). The code below shows how to do this with the variable \\(a\\):\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\n\nh = 0.0001\na = 2 + h\nb = -3.0\nc = 10 \nf = -2\ne = a*b\nd = e + c\nL2 = d * f\n\nprint(f\"L(2) = {L}\")\nprint(f\"L({a}) = {L2}\")\nprint(f\"The slope/gradient: {(L2 - L)/h}\")\n\nL(2) = -8.0\nL(2.0001) = -7.999399999999998\nThe slope/gradient: 6.000000000021544\n\n\nWe will not use this method to create our autograd, but we can use this to verify with our gradients are right."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#lets-making-a-simple-autograd",
    "href": "posts/nn-from-scratch-1/index.html#lets-making-a-simple-autograd",
    "title": "Building Neural Networks from Scratch",
    "section": "Lets making a simple AutoGrad",
    "text": "Lets making a simple AutoGrad\nThe basic idea of the AutoGrad we will make is make some very simple nodes, that represent the number in our calculation and will track all the last two nodes that make him. Basically, in \\(L = -2 \\cdot \\big((2 \\cdot 3) + 10\\big)\\), we will consider that a node can only save on number, like in the code representation\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\n\nSo, for start, lets make the basic of our class:\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0 # All nodes will start with no grad, becouse we dont know what is the grad (and for other math reaseon will explain soon)\n        self._prev = set(_children) # Dont worry about this for know, we only use set for a little bit better performance\n        self._op = _op # To save the operation, its usefull for debug\n        self.label = label # You can ignore this, its just for the graphs I make below\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\nSo with this class, we can create some Value‚Äôs, but we cant use them for anything, so lets make some operations\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label # You can ignore this, its just for the graphs I make below\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self,other), \"*\")\n        return out\n\nAnd with this simples class, we can know calc our formula (not the gradients yet)\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\nL\n\nValue(data=-8.0)\n\n\nThis calculation is known as pass forward Below, simply make a graph to visualize the operations (note that the operator is not a real node, but to visualize this it is better). The question is: how can we get the gradients for \\(L\\), \\(d\\) and \\(f\\)?\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n131990322244160\n\nc\n\ndata 10.0000\n\ngrad 0.0000\n\n\n\n131990322248864+\n\n+\n\n\n\n131990322244160-&gt;131990322248864+\n\n\n\n\n\n131990322245264\n\nL\n\ndata -8.0000\n\ngrad 0.0000\n\n\n\n131990322245264*\n\n*\n\n\n\n131990322245264*-&gt;131990322245264\n\n\n\n\n\n131990322248864\n\nd\n\ndata 4.0000\n\ngrad 0.0000\n\n\n\n131990322248864-&gt;131990322245264*\n\n\n\n\n\n131990322248864+-&gt;131990322248864\n\n\n\n\n\n131990322248336\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n131990322247616*\n\n*\n\n\n\n131990322248336-&gt;131990322247616*\n\n\n\n\n\n131990322245792\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n131990322245792-&gt;131990322247616*\n\n\n\n\n\n131990322245024\n\nf\n\ndata -2.0000\n\ngrad 0.0000\n\n\n\n131990322245024-&gt;131990322245264*\n\n\n\n\n\n131990322247616\n\ne\n\ndata -6.0000\n\ngrad 0.0000\n\n\n\n131990322247616-&gt;131990322248864+\n\n\n\n\n\n131990322247616*-&gt;131990322247616\n\n\n\n\n\n\n\n\n\nFor \\(L\\), I think it¬¥s a little obvious, its just 1, from calculus, the derivative for the function \\(f(x) = x\\) its 1. For \\(d\\) and \\(f\\), we it¬¥s simple too, from calculus, de derivative \\(f(x) = zx\\) its just z, and this is the case for both \\(d\\) and \\(f\\). See the equation below \\[\nL(d,f) = d \\cdot f\n\\] \\[\n\\frac{\\partial L}{\\partial d} = f \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial f} = d\n\\]\nSo, the gradient for \\(d\\) is the value of data in \\(f\\), and for the \\(f\\), it¬¥s the value of data in \\(d\\), in this case, the grad of \\(d\\) is -2 and the grad of \\(f\\) is 4.\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n131990322244160\n\nc\n\ndata 10.0000\n\ngrad 0.0000\n\n\n\n131990322248864+\n\n+\n\n\n\n131990322244160-&gt;131990322248864+\n\n\n\n\n\n131990322245264\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n131990322245264*\n\n*\n\n\n\n131990322245264*-&gt;131990322245264\n\n\n\n\n\n131990322248864\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n131990322248864-&gt;131990322245264*\n\n\n\n\n\n131990322248864+-&gt;131990322248864\n\n\n\n\n\n131990322248336\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n131990322247616*\n\n*\n\n\n\n131990322248336-&gt;131990322247616*\n\n\n\n\n\n131990322245792\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n131990322245792-&gt;131990322247616*\n\n\n\n\n\n131990322245024\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n131990322245024-&gt;131990322245264*\n\n\n\n\n\n131990322247616\n\ne\n\ndata -6.0000\n\ngrad 0.0000\n\n\n\n131990322247616-&gt;131990322248864+\n\n\n\n\n\n131990322247616*-&gt;131990322247616\n\n\n\n\n\n\n\n\n\nNow its start being interesting, we want to calc the gradient of \\(e\\) and \\(c\\) in relation of \\(L\\). From the chain of rule, we have the expression below: \\[\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e}\n\\] Bascially, it¬¥s saying that the gradient of \\(e\\) in relation with \\(L\\) it¬¥s just the gradient of \\(d\\) in relation with \\(L\\) time \\(e\\) in relation with \\(d\\). This mean that we only have to calc the local gradient \\(\\frac{\\partial d}{\\partial e}\\) because we already have calc the \\(\\frac{\\partial L}{\\partial d}\\) and its save in d.grad. The same with \\(c\\) it¬¥s true. So, from calculus, the gradient from an expression like \\(f(x,y) = x + y\\) its 1 for both \\(x\\) and \\(y\\). Thus, we have\n\\[\n\\frac{\\partial d}{\\partial e} = 1\n\\quad \\text{and}  \\quad\n\\frac{\\partial d}{\\partial c} = 1\n\\] \\[\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e} = -2 \\cdot 1 = -2\n\\quad \\text{and}  \\quad\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} = -2 \\cdot 1 = -2\n\\]\nThis is the strong concept that make the AutoGrad work, we only need to calc the local gradient and multiply with the gradient from the father of nodes (because the gradient of the father already is in relation with the last node).\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n131990322244160\n\nc\n\ndata 10.0000\n\ngrad -2.0000\n\n\n\n131990322248864+\n\n+\n\n\n\n131990322244160-&gt;131990322248864+\n\n\n\n\n\n131990322245264\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n131990322245264*\n\n*\n\n\n\n131990322245264*-&gt;131990322245264\n\n\n\n\n\n131990322248864\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n131990322248864-&gt;131990322245264*\n\n\n\n\n\n131990322248864+-&gt;131990322248864\n\n\n\n\n\n131990322248336\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n131990322247616*\n\n*\n\n\n\n131990322248336-&gt;131990322247616*\n\n\n\n\n\n131990322245792\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n131990322245792-&gt;131990322247616*\n\n\n\n\n\n131990322245024\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n131990322245024-&gt;131990322245264*\n\n\n\n\n\n131990322247616\n\ne\n\ndata -6.0000\n\ngrad -2.0000\n\n\n\n131990322247616-&gt;131990322248864+\n\n\n\n\n\n131990322247616*-&gt;131990322247616\n\n\n\n\n\n\n\n\n\nSo for the last two variables \\(a\\) and \\(b\\), its another multiplication, so the local grad of \\(b\\) its data of \\(a\\), and for \\(a\\) its data from \\(b\\). Put in a code, its just\n\nL.grad = 1\nd.grad = f.data\nf.grad = d.data\ne.grad = 1 * d.grad\nc.grad = 1 * d.grad\na.grad = b.data * e.grad\nb.grad = a.data * e.grad\n\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n131990322244160\n\nc\n\ndata 10.0000\n\ngrad -2.0000\n\n\n\n131990322248864+\n\n+\n\n\n\n131990322244160-&gt;131990322248864+\n\n\n\n\n\n131990322245264\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n131990322245264*\n\n*\n\n\n\n131990322245264*-&gt;131990322245264\n\n\n\n\n\n131990322248864\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n131990322248864-&gt;131990322245264*\n\n\n\n\n\n131990322248864+-&gt;131990322248864\n\n\n\n\n\n131990322248336\n\na\n\ndata 2.0000\n\ngrad 6.0000\n\n\n\n131990322247616*\n\n*\n\n\n\n131990322248336-&gt;131990322247616*\n\n\n\n\n\n131990322245792\n\nb\n\ndata -3.0000\n\ngrad -4.0000\n\n\n\n131990322245792-&gt;131990322247616*\n\n\n\n\n\n131990322245024\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n131990322245024-&gt;131990322245264*\n\n\n\n\n\n131990322247616\n\ne\n\ndata -6.0000\n\ngrad -2.0000\n\n\n\n131990322247616-&gt;131990322248864+\n\n\n\n\n\n131990322247616*-&gt;131990322247616\n\n\n\n\n\n\n\n\n\nThis is a complete backward pass manual, know we need to make a code to this automatically for us\n\nAutomatically backward pass\nTo make this automatically, we probably notted that we need to start from the last node and go in a reverse order, this is necessery because for calc the gradient of and node, we need the local gradient of the variable, and the gradient of the father, the unique exception is the last node, because its gradient always will be 1. The way will we implement this, its just make a node do the calc of his childs gradients, lets see the mul function:\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            # We use += over =, because with we use the same node two times it will be reset, and with this node influences in two other nodes, its influence in final result, are the sum of the influence on both nodes\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\nLets check with this can calc the gradients of \\(d\\) and \\(f\\)\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\nL.grad = 1\n\nL._backward()\nprint(\"Grad of d:\", d.grad)\nprint(\"Grad of f:\", f.grad)\n\nGrad of d: -2.0\nGrad of f: 4.0\n\n\nIt works! So know, its just do for add too, below are the complete Value class\n\n\nShow Python code\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n\nLets check with we can calc all the gradients:\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\nL.grad = 1\n\nL._backward() # Node L: Propagates gradient to d and f\nd._backward() # Node d: Propagates gradient to e and c\nf._backward() # Node f (Leaf): Does nothing (empty lambda)\ne._backward() # Node e: Propagates gradient to a and b\nc._backward() # Node c (Leaf): Does nothing\nb._backward() # Node b (Leaf): Does nothing\na._backward() # Node a (Leaf): Does nothing\n\nprint(f\"L data: {L.data}\")\nprint(\"-\" * 20)\nprint(f\"Grad of L: {L.grad}\") # Should be 1\nprint(f\"Grad of f: {f.grad}\") # Should be 4.0 (d.data)\nprint(f\"Grad of d: {d.grad}\") # Should be -2.0 (f.data)\nprint(f\"Grad of c: {c.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of e: {e.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of b: {b.grad}\") # Should be -4.0 (a.data * e.grad -&gt; 2 * -2)\nprint(f\"Grad of a: {a.grad}\") # Should be 6.0 (b.data * e.grad -&gt; -3 * -2)\n\nL data: -8.0\n--------------------\nGrad of L: 1\nGrad of f: 4.0\nGrad of d: -2.0\nGrad of c: -2.0\nGrad of e: -2.0\nGrad of b: -4.0\nGrad of a: 6.0\n\n\nIt worked perfectly, know we only need to make a funcion that calls all _backward() in all nodes recursively. Therefore we will use an algorithm for generating the topological order of the graph. That is, a linear order in which we can execute the nodes without causing any kind of dependency problem. Explaining this algorithm in depth is outside the scope of this project, but it is not that complicated, see below:\n\n# ... All methods of Value class\ndef backward(self):\n    self.grad = 1\n    # Montar ordem topologica\n    topo = []\n    visited = set()\n    def build_topo(v):\n        if v not in visited:\n            visited.add(v)\n            for child in v._prev:\n                build_topo(child)\n            topo.append(v)\n    build_topo(self)\n\n    for node in reversed(topo):\n        node._backward()\n\nKnow, lets test our backward function\n\n\nShow Python code\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        self.grad = 1\n        # create a topological order\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        for node in reversed(topo):\n            node._backward()\n\n\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\n\nL.backward() \n\n\nprint(f\"L data: {L.data}\")\nprint(\"-\" * 20)\nprint(f\"Grad of L: {L.grad}\") # Should be 1\nprint(f\"Grad of f: {f.grad}\") # Should be 4.0 (d.data)\nprint(f\"Grad of d: {d.grad}\") # Should be -2.0 (f.data)\nprint(f\"Grad of c: {c.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of e: {e.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of b: {b.grad}\") # Should be -4.0 (a.data * e.grad -&gt; 2 * -2)\nprint(f\"Grad of a: {a.grad}\") # Should be 6.0 (b.data * e.grad -&gt; -3 * -2)\n\nL data: -8.0\n--------------------\nGrad of L: 1\nGrad of f: 4.0\nGrad of d: -2.0\nGrad of c: -2.0\nGrad of e: -2.0\nGrad of b: -4.0\nGrad of a: 6.0\n\n\nIt¬¥s worked perfectly again, know we already have a functional AutoGrad system, know, it¬¥s just need to add more operations and we will be capable of create our propely framework of Deep Learning using our own AutoGrad. Before going to the next part, see the code bellow, its the same that we create together, but with some little changes, try to figure out what they‚Äôre for (Hint, they‚Äôre important for the next part)\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n    \n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += (other * self.data**(other-1)) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def __neg__(self): \n        return self * -1\n\n    def __sub__(self, other): \n        return self + (-other)\n\n    def __radd__(self, other): \n        return self + other\n\n    def backward(self):\n        self.grad = 1\n        # create a topological order\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        for node in reversed(topo):\n            node._backward()"
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#lets-make-our-deep-learning-framework",
    "href": "posts/nn-from-scratch-1/index.html#lets-make-our-deep-learning-framework",
    "title": "Building Neural Networks from Scratch",
    "section": "Lets make our Deep Learning framework",
    "text": "Lets make our Deep Learning framework\n\nBuilding a Neuron\nSo, for now, we have already created our own engine to calculate our gradients. Thus, lets start making a simple artificial neuron. If you don‚Äôt know or don‚Äôt remember how a neuron work, basically it receives inputs, multiply each one by some weight, sum all values, and last, pass this value in a non-linear function\n\n\n\n\n\n\nFigure¬†1: An Artificial Neuron\n\n\n\nTo start, we can make a simple class for our neuron:\n\nimport random\nclass Neuron:\n    def __init__(self, input_num, non_linear=True):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(input_num)] # This basicaly creates a list of Value's from a uniform distribuition with lenth equals to input_num\n        self.b = Value(random.uniform(-1,1)) # This is for the bias, explain deeply the importance of this is out of scope, but you can think bias its just a special weight that dosnt multiply the inputs, just sum in the sum part.\n\n    # Call is a special function in python, its like __add__, the sintax for use this is when you just put () after the object, like range(x)\n    def __call__(self,x):\n        out = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) # This just do the calculation of a neuron until the sum part\n        return out\n    \n    def parameters(self):\n        return self.w + [self.b] # this function just return all the parameters of our neuron\n\nAnd, its just it, a completely and functional artificial neuron.\nKnow, lets test if this are working\n\nn = Neuron(2)\nx = [2, 3, 4]\nout = n(x)\nprint(out, n.parameters())\n\nValue(data=2.7606105336395843) [Value(data=0.34610789511609674), Value(data=0.8252449036669098), Value(data=-0.4073399675933387)]\n\n\nWorks, lets move on.\n\n\nBuilding a Layer\nSo, we already create an simple Neuron, now, we just need to line them up for make a Layer.\n\nclass Layer:\n    def __init__(self, input_num, output_num, non_linear=True):\n        self.neurons = [Neuron(input_num) for _ in range(output_num)] # Create a list of neurons that accept input_num variables in input, and will generate output_num outputs\n        self.non_linear = non_linear\n\n    # Just process all operations in all neurons with the input\n    def __call__(self,x):\n        outs = [n(x) for n in self.neurons]\n\n        if self.non_linear:\n            outs = [out.tahn() for out in outs]\n\n        return outs[0] if len(outs) == 1 else outs\n    \n    def parameters(self):\n        out = []\n        for neuron in self.neurons:\n            out.extend(neuron.parameters()) # Add the values in an unique list\n        return out\n\nFor this Layer class, we need to implement the tahn (tangent hyperbolic) function. It‚Äôs formula is: \\[\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nNote that there is 3 operations that we don‚Äôt implement to make this, so, lets implement them. For know, I presume that you understand how to implement this, so, try your self, you can search in google for this, just don‚Äôt use an llm.\n\n\nShow solution:\nimport math\n# Its not really necessary to add exp, subtraction or divided, but its good with you do üòä\ndef tahn(self):\n    x = self.data\n    t = (math.exp(x) - math.exp(-x))/ (math.exp(x) + math.exp(-x))\n    out = Value(t, (self,), \"tahn\")\n\n    # We can derivate, or just pick up on google\n    def _backwards():\n        self.grad += (1 - t**2) * out.grad\n    \n    out._backward = _backward\n    return out\n\n\nLet‚Äôs test if this work\n\nx = [2, 3, 4]\n\nlayer = Layer(3,3)\nlayer(x), layer.parameters()\n\n([Value(data=0.9996646615551876),\n  Value(data=0.9999791585105655),\n  Value(data=-0.9747811998177511)],\n [Value(data=-0.2399760854356343),\n  Value(data=0.5672209838656024),\n  Value(data=0.6526425611040982),\n  Value(data=0.5143938500523895),\n  Value(data=0.7498233764763123),\n  Value(data=-0.20504799838269894),\n  Value(data=0.9718665298515214),\n  Value(data=0.9638819394987441),\n  Value(data=-0.9427159532454912),\n  Value(data=-0.5007685680188607),\n  Value(data=0.19206424478156947),\n  Value(data=0.4391690659468055)])\n\n\nWork well, this is pretty simple too. Now let‚Äôs move to the last part, create an complete Artificial Neural Network\n\n\nBuilding an MLP\nFor last, we will create a Multilayer Perceptron, for this, we just need to to line layer up.\n\nclass MLP:\n    def __init__(self, input_num, outputs_nums):\n        self.layers = []\n        values = [input_num] + outputs_nums\n        for id in range(len(outputs_nums)-1): # This work for create an unique list used to describe input and output for all layers\n            self.layers.append(Layer(values[id], values[id+1]))\n        self.layers.append(Layer(values[-2], values[-1], non_linear=False))\n\n    def __call__(self, X):\n        for layer in self.layers: # This works because with exception from the first, each layer just receive the input from the last layer\n            X = layer(X)\n        return X\n\n    def parameters(self):\n        out = []\n        for layer in self.layers:\n            out.extend(layer.parameters())\n        return out\n\nAnd, its finish, just need to test now\n\nnn = MLP(3, [6,6,1])\nx = [2, 3, 4]\n\nnn(x) # You can test the parameters\n\nValue(data=-0.7919551271747531)\n\n\n\n\nTraining our neural network\nIn the last part, we create a MLP, but we don‚Äôt fit them in any data. Lets create a function and values with some noise\n\nimport random\n\ndef f(x,y,z):\n    return 0.04*x**2 + 0.07*y*x - z + random.gauss(0, 1)/5\n\nX = []\ny = []\nfor i in range(100):\n    X.append([random.uniform(-5,5) for _ in range(3)])\n    y.append(f(X[i][0],X[i][1],X[i][2]))\n\nTo implement the train loop, we will need one more thing, there is an loss functions, this is just an metric that describes how well our model are fitting in data. For these case, we will use Mean Squared Error, so the steps for the training loop, its, calc the prediction, calc the loss, calc the gradients, subtract the gradients from the weights, reset the gradients values (our code accumulate the gradients) and do it again, and again.\n\nann = MLP(3, [8, 8, 1]) \n \nfor i in range(50): # Our code will do 50 epochs of training \n    y_pred = [ann(x) for x in X] # Our model only accept one prediction per time\n    loss = sum((pred-origin)**2 for pred,origin in zip(y_pred, y)) \n\n    loss.backward() # Calc of gradients \n\n    for p in ann.parameters():\n        p.data = p.data - 0.001*p.grad # Updating weights, we multiply the gradient by a small number called learning rate, we do this for not have problems with model convergence\n        p.grad = 0\n    print(f\"Epoch {i}, Loss: {loss}\")\n\nEpoch 0, Loss: Value(data=1173.927825101448)\nEpoch 1, Loss: Value(data=479.3293651535565)\nEpoch 2, Loss: Value(data=274.5878032027161)\nEpoch 3, Loss: Value(data=179.24830078948708)\nEpoch 4, Loss: Value(data=145.57014093033285)\nEpoch 5, Loss: Value(data=135.2468547794671)\nEpoch 6, Loss: Value(data=128.96468805258488)\nEpoch 7, Loss: Value(data=123.73377230062457)\nEpoch 8, Loss: Value(data=119.24956713114472)\nEpoch 9, Loss: Value(data=115.34916728015705)\nEpoch 10, Loss: Value(data=111.9069266409732)\nEpoch 11, Loss: Value(data=108.81655285791855)\nEpoch 12, Loss: Value(data=105.98954851293747)\nEpoch 13, Loss: Value(data=103.35366087728872)\nEpoch 14, Loss: Value(data=100.85140836678144)\nEpoch 15, Loss: Value(data=98.43793989880551)\nEpoch 16, Loss: Value(data=96.07888048817817)\nEpoch 17, Loss: Value(data=93.7488216361225)\nEpoch 18, Loss: Value(data=91.43017742079934)\nEpoch 19, Loss: Value(data=89.11255823869828)\nEpoch 20, Loss: Value(data=86.79210670629311)\nEpoch 21, Loss: Value(data=84.4706497341805)\nEpoch 22, Loss: Value(data=82.15431043581388)\nEpoch 23, Loss: Value(data=79.8516216415455)\nEpoch 24, Loss: Value(data=77.57130376585995)\nEpoch 25, Loss: Value(data=75.32018414386577)\nEpoch 26, Loss: Value(data=73.10179409190116)\nEpoch 27, Loss: Value(data=70.91631914225951)\nEpoch 28, Loss: Value(data=68.76231336621191)\nEpoch 29, Loss: Value(data=66.64041601255104)\nEpoch 30, Loss: Value(data=64.55760585859981)\nEpoch 31, Loss: Value(data=62.52884802189379)\nEpoch 32, Loss: Value(data=60.57154326368767)\nEpoch 33, Loss: Value(data=58.6963545301056)\nEpoch 34, Loss: Value(data=56.903006792033935)\nEpoch 35, Loss: Value(data=55.185875697576535)\nEpoch 36, Loss: Value(data=53.541247825483175)\nEpoch 37, Loss: Value(data=51.98022532924802)\nEpoch 38, Loss: Value(data=50.57101858010987)\nEpoch 39, Loss: Value(data=49.60115194381445)\nEpoch 40, Loss: Value(data=50.452864150600426)\nEpoch 41, Loss: Value(data=58.13598315203789)\nEpoch 42, Loss: Value(data=99.44026776624415)\nEpoch 43, Loss: Value(data=173.6608093660121)\nEpoch 44, Loss: Value(data=290.50097477153923)\nEpoch 45, Loss: Value(data=111.89952934255297)\nEpoch 46, Loss: Value(data=67.26368862356497)\nEpoch 47, Loss: Value(data=55.093670302415795)\nEpoch 48, Loss: Value(data=49.50636689868851)\nEpoch 49, Loss: Value(data=46.54826277713668)\n\n\nWork, but not very well, it‚Äôs possible to improve this changing the activation function by an ReLU. But for our propose, it‚Äôs good enough. Lets just see the values coming from our model\n\ny_pred = ann(X[1])\nprint(f\"Real value: {y[1]}\\nPredict value: {y_pred}\") \n\nReal value: 3.561263195532192\nPredict value: Value(data=3.0152658379527537)"
  },
  {
    "objectID": "posts/makemore-1/index.html",
    "href": "posts/makemore-1/index.html",
    "title": "Building an Autoregressive Neural Network",
    "section": "",
    "text": "In this post, we will implement an Autoregressive Neural Network from scratch, relying solely on the PyTorch tensor class. We assume prior familiarity with Neural Networks; however, if your knowledge feels a bit rusty or you need a refresher, I recommend reading this post beforehand Building Neural Networks from Scratch.\nThe main reason for this is to learn how an Autoregressive NN works to generate words, for this, I‚Äôm drawing on Andrej Karpathy‚Äôs video series about makemore, a network capable of creating more words of the same type, so if you train with names, it generates more proper names it generates more words that remember proper names, and so on with anything that is formed by letters.\nIn this post, I will cover how to make a simple model for our baseline, and how to implement a model with MLP and compare them."
  },
  {
    "objectID": "posts/makemore-1/index.html#brief-summary",
    "href": "posts/makemore-1/index.html#brief-summary",
    "title": "Building an Autoregressive Neural Network",
    "section": "",
    "text": "In this post, we will implement an Autoregressive Neural Network from scratch, relying solely on the PyTorch tensor class. We assume prior familiarity with Neural Networks; however, if your knowledge feels a bit rusty or you need a refresher, I recommend reading this post beforehand Building Neural Networks from Scratch.\nThe main reason for this is to learn how an Autoregressive NN works to generate words, for this, I‚Äôm drawing on Andrej Karpathy‚Äôs video series about makemore, a network capable of creating more words of the same type, so if you train with names, it generates more proper names it generates more words that remember proper names, and so on with anything that is formed by letters.\nIn this post, I will cover how to make a simple model for our baseline, and how to implement a model with MLP and compare them."
  },
  {
    "objectID": "posts/makemore-1/index.html#setup",
    "href": "posts/makemore-1/index.html#setup",
    "title": "Building an Autoregressive Neural Network",
    "section": "Setup",
    "text": "Setup\nFirst, you need to download PyTorch and the dataset. For PyTorch, just download in the official site https://pytorch.org/get-started/locally/. Now, for the dataset, you can create your own with random names that you can think, but It‚Äôs much easier just download the names.txt dataset from the Andrej repository https://github.com/karpathy/makemore/blob/master/names.txt."
  },
  {
    "objectID": "posts/makemore-1/index.html#creating-a-baseline",
    "href": "posts/makemore-1/index.html#creating-a-baseline",
    "title": "Building an Autoregressive Neural Network",
    "section": "Creating a baseline",
    "text": "Creating a baseline\nIn propose of this, it‚Äôs just to create the most simple and naive model. It‚Äôs important because we need some baseline to compare with our future models, so we will create a model called bigram, the logic is just to look to the last character. Note that you will use just one character of context for our model, and we will consider that the most small part of our word is a character, for models like chatGPT, they don‚Äôt use characters, they use combinations of characters similar to syllables.\nSo, to start, we need first import our dataset and PyTorch\n\nimport torch\n \n# Basically makes a list of all the names\nnames = open(\"names.txt\", \"r\").read().splitlines() \nnames[:5]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\n\n\nMost part of models usually can‚Äôt handle with characters, so it‚Äôs useful to convert this letters in numbers in some way. For this, there are many possibles, but I will use just a simple dictionary to convert them. But we\n\nchars = sorted(list(set(\"\".join(names)))) # Creates an ordered list with all letters in our dataset\ncharToInt = {s:i+1 for i,s in enumerate(chars)} # Creates a dict to convert chars to int, we add one in the value, because of the bellow line\ncharToInt[\".\"] = 0 # I will explain later why we need a special character\nprint(charToInt)\n\n{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n\n\nJust to get it ready, if we convert to int, so we can read it at the end, we will need an intToChar converter, so let‚Äôs get it ready\n\nintToChar = {s:i for i,s in charToInt.items()}\nprint(intToChar)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n\n\nKnow, for our model, we need to calculate the total number that each sequence occurs, like, with we start with letter ‚Äúa‚Äù, how many times occurs that ‚Äúm‚Äù is the next character. And it‚Äôs for this that we need and special characters, because we always need something to start, after all, the autoregressive model logic and take the output of the model and put it in its input, so we need an initial input. In our case, we will use ‚Äú.‚Äù as the symbol to start a name/words and to stop word (without a final symbol, it would generate forever). To make more clear, see the code bellow\n\nN = torch.zeros((27,27)).int()\n\nfor name in names:\n    chars = [\".\"] + list(name) + [\".\"] # turn the name in a list of characters and add \".\" \n    for ch1,ch2 in zip(chars, chars[1:]): # In each loop, pick up one letter in ch1, and the next in ch2\n        id1, id2 = charToInt[ch1], charToInt[ch2]\n        N[id1, id2] += 1\n\nBasically, this count how often some sequence of characters occurs, like the most common letter sequence is ‚Äún‚Äù follow by ‚Äú.‚Äù, this mean, that the most commum letter to finish a name in our dataset it‚Äôs ‚Äún‚Äù. If you run with all the names, you can use the code bellow to find the most common occurrences\n\nid1, id2 = (N == N.max()).nonzero(as_tuple=True) # Creates a boolean matrix that only it's True in the max value, than return a tuple where its true\nprint(intToChar[id1.item()], \"--&gt;\", intToChar[id2.item()],\"occurs \", N.max().item())\n\nn --&gt; . occurs  6763\n\n\nSo let‚Äôs see how our bigrams are distributed\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize= (16,16))\nplt.imshow(N, cmap=\"Blues\")\nfor i in range(27):\n    for j in range(27):\n        chstr = intToChar[i] + intToChar[j]\n        plt.text(j,i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n        plt.text(j,i, N[i,j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n    \nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\nOne thing very interesting you can note, it‚Äôs that have many combinations that don‚Äôt exist, like ‚Äúbk‚Äù or ‚Äúgc‚Äù. This makes it impossible for our model to generate a name with this combination, it is ok to leave it like this, but it would be a good practice to add 1 in all values, thus ensuring that at least there is the minimal possibility of generating a rare sequence\n\nN = N + 1 \n\n\n\n\n\n\n\n\n\n\nSo, lets transform our probability matrix\n\nP = N\nP = P / P.sum(dim=1, keepdims=True)\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize= (16,16))\nplt.imshow(P, cmap=\"Blues\")\nfor i in range(27):\n    for j in range(27):\n        chstr = intToChar[i] + intToChar[j]\n        plt.text(j,i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n        plt.text(j,i, f\"{P[i,j].item():.3f}\", ha=\"center\", va=\"top\", color=\"gray\")\n    \nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\nSome probabilities stay in 0 because the visualization it‚Äôs limited to 3 decimal numbers. Now we already have our model, it‚Äôs just our probability matrix P, bellow I will show how to use it.\n\nfor i in range(10):\n    out = []\n    init = 0\n    while True:\n        id = torch.multinomial(P[init], num_samples=1, replacement=True).item()\n\n        if id == 0: \n            break\n\n        out.append(intToChar[id])\n        init = id\n    print(\"\".join(out)) \n\nseren\nti\nkegwa\nshamidelyl\nseazay\njome\nbashuligey\nswadellandeckaramauaigen\nn\nadeloc\n\n\nNot good at all, but it‚Äôs correct, with you think the model it‚Äôs just saying random letters, see the code bellow, were any letter has the same probability\n\nN = torch.ones((27,27))\nP = N\nP = P / P.sum()\n\nfor i in range(10):\n    out = []\n    init = 0\n    while True:\n        id = torch.multinomial(P[init], num_samples=1, replacement=True).item()\n\n        if id == 0: \n            break\n\n        out.append(intToChar[id])\n        init = id\n    print(\"\".join(out)) \n\ngsogwlixwdmsfjehnpius\nhoeauujelywlnlgwgubsygmanhmenfddnsnoplmlmzqrglwhxcobnbbfhqegtxbzuxhquw\njllihyagglxhczwjlaaffyvbjgrglhodtmu\njzcz\nywbqpdsvspemyipgtbydx\n\nicpfhuxyx\nprxo\nixgtidexdbbolsimavyulqhsvggstbwjjyang\nqi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Building an Autoregressive Neural Network\n\n\nPart 1\n\n\n\nscratch\n\n\nen\n\n\ncode\n\n\nnn\n\n\n\n\n\n\n\n\n\nJan 28, 2026\n\n\nLuca WB\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Neural Networks from Scratch\n\n\n\n\n\n\nscratch\n\n\nen\n\n\ncode\n\n\nmath\n\n\nnn\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nLuca WB\n\n\n\n\n\n\n\n\n\n\n\n\nO B√°sico de Machine Unlearning em LLMs\n\n\n\n\n\n\nunlearning\n\n\npt\n\n\ncode\n\n\nquantization\n\n\nmath\n\n\nllms\n\n\n\n\n\n\n\n\n\nDec 19, 2025\n\n\nLuca WB\n\n\n\n\n\n\nNo matching items"
  }
]