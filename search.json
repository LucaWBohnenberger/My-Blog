[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Luca Wolffenbuttel Bohnenberger",
    "section": "",
    "text": "I am a Data Science and Artificial Intelligence student at PUCRS and a researcher at NAIA, where I focus on Applied Research in Large Language Models (LLMs).\n\n\nThis space serves as a repository for my studies and technical reflections. My main goal is to document my learning journey in a didactic way, transforming dense academic notes into clear, structured, and accessible content.\nWhether you are a fellow student or a researcher, I hope my notes can help demystify the ‚Äúblack box‚Äù of AI for you as they do for me.\n\n‚ÄúWhat I cannot create, I do not understand.‚Äù ‚Äì Richard Feynman"
  },
  {
    "objectID": "about.html#welcome-to-my-digital-notebook",
    "href": "about.html#welcome-to-my-digital-notebook",
    "title": "Luca Wolffenbuttel Bohnenberger",
    "section": "",
    "text": "I am a Data Science and Artificial Intelligence student at PUCRS and a researcher at NAIA, where I focus on Applied Research in Large Language Models (LLMs).\n\n\nThis space serves as a repository for my studies and technical reflections. My main goal is to document my learning journey in a didactic way, transforming dense academic notes into clear, structured, and accessible content.\nWhether you are a fellow student or a researcher, I hope my notes can help demystify the ‚Äúblack box‚Äù of AI for you as they do for me.\n\n‚ÄúWhat I cannot create, I do not understand.‚Äù ‚Äì Richard Feynman"
  },
  {
    "objectID": "posts/machine-unlearning/index.html",
    "href": "posts/machine-unlearning/index.html",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "",
    "text": "Machine Unlearning consiste na tecnica de tentar fazer modelos desaprenderem algo, seja um conhecimento especifico, uma l√≥gica, ou uma habilidade inteira, esse post ser√° especifico sobre bases de machine unlearning em LLMs\nDe maneira geral, ao tentar fazer uma llm esquecer de algo, passamos por um processo de fine tuning utilizando um uma loss especifica para isso junto com dois datasets.\nA formula abaixo mostra como se calcula a Loss para um problema de MU (Machine Unlearning) \\[\n\\min_{\\theta} \\underbrace{\\mathbb{E}_{(x, y_f) \\in D_f} [\\ell(y_f | x; \\theta)]}_{\\text{Forget}} + \\lambda \\underbrace{\\mathbb{E}_{(x, y) \\in D_r} [\\ell(y | x; \\theta)]}_{\\text{Retain}}\n\\]\nVamos dissecar essa formula para n√£o restar duvidas, a primeira parte, ‚ÄúForget‚Äù, √© a fun√ß√£o que loss que penaliza o modelo por ele dar a resposta original (ou qualquer uma que n√£o seja a desejada) dado o input \\(x\\) e os pesos \\(\\theta\\), isso √© o que faz o Unlearning, note que ele usa o dataset \\(D_f\\), que consiste em inputs e outputs desejaveis p√≥s-unlearning, ou seja, se eu quiser apagar o conhecimento do harry potter, a saida desejavel para a pergunta ‚ÄúComo Harry Potter encontrou a pedra filosofal no primeiro livro?‚Äù deve ser algo como ‚ÄúN√£o posso falar sobre isso‚Äù, ou ent√£o ‚ÄúN√£o sei‚Äù.\nA segunda parte, ‚ÄúRetain‚Äù, serve para que n√£o ocorra um esquecimento generalizado do modelo (como por exemplo ele desaprender portugues), aqui usamos o dataset \\(D_r\\), que tambem consiste em input output, porem aqui o output √© a saida original do modelo, \\(\\lambda\\) √© um hiperparametro que regula o quanto o modelo deve priorizar manter o valor original dos pesos, ou seja, \\(\\lambda\\) baixo, modelo pode esquecer de mais, \\(\\lambda\\) alto, modelo pode n√£o esquecer o suficente.\nO \\(E\\) significa esperan√ßa (a m√©dia), no caso n√£o usamos os resultados singulares de cada linha do dataset, e sim a m√©dia da loss deles.\n\\(\\min_{\\theta}\\) √© simplesmente a nota√ß√£o que expressa que queremos minimizar isso modificando \\(\\theta\\), em si n√£o quer dizer nada matematicamente, isso √© expresso subtraindo a loss dos valores dos pesos."
  },
  {
    "objectID": "posts/machine-unlearning/index.html#ideia-geral",
    "href": "posts/machine-unlearning/index.html#ideia-geral",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "",
    "text": "Machine Unlearning consiste na tecnica de tentar fazer modelos desaprenderem algo, seja um conhecimento especifico, uma l√≥gica, ou uma habilidade inteira, esse post ser√° especifico sobre bases de machine unlearning em LLMs\nDe maneira geral, ao tentar fazer uma llm esquecer de algo, passamos por um processo de fine tuning utilizando um uma loss especifica para isso junto com dois datasets.\nA formula abaixo mostra como se calcula a Loss para um problema de MU (Machine Unlearning) \\[\n\\min_{\\theta} \\underbrace{\\mathbb{E}_{(x, y_f) \\in D_f} [\\ell(y_f | x; \\theta)]}_{\\text{Forget}} + \\lambda \\underbrace{\\mathbb{E}_{(x, y) \\in D_r} [\\ell(y | x; \\theta)]}_{\\text{Retain}}\n\\]\nVamos dissecar essa formula para n√£o restar duvidas, a primeira parte, ‚ÄúForget‚Äù, √© a fun√ß√£o que loss que penaliza o modelo por ele dar a resposta original (ou qualquer uma que n√£o seja a desejada) dado o input \\(x\\) e os pesos \\(\\theta\\), isso √© o que faz o Unlearning, note que ele usa o dataset \\(D_f\\), que consiste em inputs e outputs desejaveis p√≥s-unlearning, ou seja, se eu quiser apagar o conhecimento do harry potter, a saida desejavel para a pergunta ‚ÄúComo Harry Potter encontrou a pedra filosofal no primeiro livro?‚Äù deve ser algo como ‚ÄúN√£o posso falar sobre isso‚Äù, ou ent√£o ‚ÄúN√£o sei‚Äù.\nA segunda parte, ‚ÄúRetain‚Äù, serve para que n√£o ocorra um esquecimento generalizado do modelo (como por exemplo ele desaprender portugues), aqui usamos o dataset \\(D_r\\), que tambem consiste em input output, porem aqui o output √© a saida original do modelo, \\(\\lambda\\) √© um hiperparametro que regula o quanto o modelo deve priorizar manter o valor original dos pesos, ou seja, \\(\\lambda\\) baixo, modelo pode esquecer de mais, \\(\\lambda\\) alto, modelo pode n√£o esquecer o suficente.\nO \\(E\\) significa esperan√ßa (a m√©dia), no caso n√£o usamos os resultados singulares de cada linha do dataset, e sim a m√©dia da loss deles.\n\\(\\min_{\\theta}\\) √© simplesmente a nota√ß√£o que expressa que queremos minimizar isso modificando \\(\\theta\\), em si n√£o quer dizer nada matematicamente, isso √© expresso subtraindo a loss dos valores dos pesos."
  },
  {
    "objectID": "posts/machine-unlearning/index.html#otimizadores",
    "href": "posts/machine-unlearning/index.html#otimizadores",
    "title": "O B√°sico de Machine Unlearning em LLMs",
    "section": "Otimizadores",
    "text": "Otimizadores\nAparentemente otimizadoraes de segunda ordem que estimam a diagonal de uma hessiana (como a Sophia) funcionam melhor para unlearning do que otimizadores de primeira ordem (SGD, Adam, RMSprop etc)"
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html",
    "href": "posts/nn-from-scratch-1/index.html",
    "title": "Building Neural Networks from Scratch",
    "section": "",
    "text": "The objective from this page to undestand how to implement a neural network from scratch without any external libraries, this page consider that you already have some knwlodge of Artificial Neural Networks. The main reason for this is just to make more compreensible the black box of Neural Networks. So, to this project, the main resource (but not the unique) are the series of videos from Andrej Karpathy. In this first part, I will cover how to implement AutoGrad for do backpropagation. At the end of this post, yo will cable of create MLPs without any external library."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#brief-summary",
    "href": "posts/nn-from-scratch-1/index.html#brief-summary",
    "title": "Building Neural Networks from Scratch",
    "section": "",
    "text": "The objective from this page to undestand how to implement a neural network from scratch without any external libraries, this page consider that you already have some knwlodge of Artificial Neural Networks. The main reason for this is just to make more compreensible the black box of Neural Networks. So, to this project, the main resource (but not the unique) are the series of videos from Andrej Karpathy. In this first part, I will cover how to implement AutoGrad for do backpropagation. At the end of this post, yo will cable of create MLPs without any external library."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#basic-knowledge-of-derivative",
    "href": "posts/nn-from-scratch-1/index.html#basic-knowledge-of-derivative",
    "title": "Building Neural Networks from Scratch",
    "section": "Basic knowledge of derivative",
    "text": "Basic knowledge of derivative\nSo, from the start, to make sense at how an NN train and learn something, you first need a very good understanding around the meaning of derivative operations. A derivative is an operation that gives us a formula that describes the slope of a function as it modifies a variable, but for out prupose, we will only work with functions that generate linear derivatives. Thus, for the function \\(f(x) = 3x^2 - 4x + 5\\), see the graph below.\n\n\nShow Python code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\n# Generate x values\nx = np.linspace(-5, 5, 400)\ny = f(x)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r\"$f(x) = 3x^2 - 4x + 5$\")\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Graph of the function\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis function is easy to undestand and derivate analytic, the derivate is \\(\\frac{df(x)}{dx} = 6x - 4\\). Plotting both function and his derivate, we get the graphic below.\n\n\nShow Python code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\ndef df(x):\n    return 6*x - 4\n\n# Generate x values\nx = np.linspace(-5, 5, 400)\ny = f(x)\ny2 = df(x)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r\"$f(x) = 3x^2 - 4x + 5$\")\nplt.plot(x, y2, label=r\"$df(x) = 6*x - 4$\")\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Graph of the function\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe basic idea is that with we want to minimize the value of a function (main objective in deep learning), we just need to see the value of a derivative in some point A, that give us all the information we need to go to the minimum spot. Just do some numerical exemple, in the graph above, note that the minimum spot is in som evalue around 0 and 2, more close to 0 (precisaly 2/3), so, just pick some rondom number, like -2, the value of \\(f(x)\\) with -2 is 25, and the derivative is -16. The number -16 represent the rate at which the function varies for each increase in the value of x at that point in specific. So, with we increse the value of X a little bit, we can lower the value of X, so probably, the \\(f(-1.999)\\) give us a lower value than \\(f(-2)\\)\n\ndef f(x):\n    return 3*x**2 - 4*x + 5\n\nprint(\"f(-2)=\",f(-2))\nprint(\"f(-1.999)=\",f(-1.999))\n\nf(-2)= 25\nf(-1.999)= 24.984003\n\n\nThis is the general idea of how we can minimize some function, that is also the main idea of how gradient descendent works."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#how-to-estimate-gradients",
    "href": "posts/nn-from-scratch-1/index.html#how-to-estimate-gradients",
    "title": "Building Neural Networks from Scratch",
    "section": "How to estimate gradients",
    "text": "How to estimate gradients\nIn general, to make an framework for working with nn, its just an AutoGrad (a tool that can do diferatiation automatically) and some fancy stuff for make more pratical.\nFor start, lets make things the most simple for now. Our goal its make an class that can calc for us all the gradietns (its the same as an derivative) from the function \\(L = -2 \\cdot \\big((2 \\cdot 3) + 10\\big)\\). But we dont are confortable derivate something with just numbers, so lets consider in this way the function:\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\nL\n\n-8.0\n\n\nJust to make clear, the knowledge that we want with this, is ow much changes in the final result, increase the values of any of the variables a little To get the gradients, we can use an aproximate that consists in adding a very small number \\(h\\) is all of the values, than subtract the new with the original, and divide by the \\(h\\). The code below shows how to do this with the variable \\(a\\):\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\n\nh = 0.0001\na = 2 + h\nb = -3.0\nc = 10 \nf = -2\ne = a*b\nd = e + c\nL2 = d * f\n\nprint(f\"L(2) = {L}\")\nprint(f\"L({a}) = {L2}\")\nprint(f\"The slope/gradient: {(L2 - L)/h}\")\n\nL(2) = -8.0\nL(2.0001) = -7.999399999999998\nThe slope/gradient: 6.000000000021544\n\n\nWe will not use this method to create our autograd, but we can use this to verify with our gradients are right."
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#lets-making-a-simple-autograd",
    "href": "posts/nn-from-scratch-1/index.html#lets-making-a-simple-autograd",
    "title": "Building Neural Networks from Scratch",
    "section": "Lets making a simple AutoGrad",
    "text": "Lets making a simple AutoGrad\nThe basic idea of the AutoGrad we will make is make some very simple nodes, that represent the number in our calculation and will track all the last two nodes that make him. Basically, in \\(L = -2 \\cdot \\big((2 \\cdot 3) + 10\\big)\\), we will consider that a node can only save on number, like in the code represantation\n\na = 2\nb = -3.0\nc = 10\nf = -2\ne = a*b\nd = e + c\nL = d * f\n\nSo, for start, lets make the basic of our class:\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0 # All nodes will start with no grad, becouse we dont know what is the grad (and for other math reaseon will explain soon)\n        self._prev = set(_children) # Dont worry about this for know, we only use set for a little bit better performance\n        self._op = _op # To save the operation, its usefull for debug\n        self.label = label # You can ignore this, its just for the graphs I make below\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\nSo with this class, we can create some Value‚Äôs, but we cant use them for anything, so lets make some operations\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label # You can ignore this, its just for the graphs I make below\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        return out\n    \n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self,other), \"*\")\n        return out\n\nAnd with this simples class, we can know calc our formula (not the gradients yet)\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\nL\n\nValue(data=-8.0)\n\n\nThis calculation moting the structure of a graph is known as pass forward Below, a just make an graph to visualize the operations (note that the operator is not an actual node, but for vizualize this is better), The question is, how we can get the gradients for \\(L\\), \\(d\\) and \\(f\\)?\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n125139691820544\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n125139691819536*\n\n*\n\n\n\n125139691820544-&gt;125139691819536*\n\n\n\n\n\n125139691821072\n\nd\n\ndata 4.0000\n\ngrad 0.0000\n\n\n\n125139691821792*\n\n*\n\n\n\n125139691821072-&gt;125139691821792*\n\n\n\n\n\n125139691821072+\n\n+\n\n\n\n125139691821072+-&gt;125139691821072\n\n\n\n\n\n125139691819536\n\ne\n\ndata -6.0000\n\ngrad 0.0000\n\n\n\n125139691819536-&gt;125139691821072+\n\n\n\n\n\n125139691819536*-&gt;125139691819536\n\n\n\n\n\n125139691821792\n\nL\n\ndata -8.0000\n\ngrad 0.0000\n\n\n\n125139691821792*-&gt;125139691821792\n\n\n\n\n\n125139691816224\n\nc\n\ndata 10.0000\n\ngrad 0.0000\n\n\n\n125139691816224-&gt;125139691821072+\n\n\n\n\n\n125139691828512\n\nf\n\ndata -2.0000\n\ngrad 0.0000\n\n\n\n125139691828512-&gt;125139691821792*\n\n\n\n\n\n125139691820352\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n125139691820352-&gt;125139691819536*\n\n\n\n\n\n\n\n\n\nFor \\(L\\), I think it¬¥s a little obvious, its just 1, from calculus, the derivative for the function \\(f(x) = x\\) its 1. For \\(d\\) and \\(f\\), we it¬¥s simple too, from calculus, de derivative \\(f(x) = zx\\) its just z, and this is the case for both \\(d\\) and \\(f\\). See the equation below \\[\nL(d,f) = d \\cdot f\n\\] \\[\n\\frac{\\partial L}{\\partial d} = f \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial f} = d\n\\]\nSo, the gradient for \\(d\\) is the value of data in \\(f\\), and for the \\(f\\), it¬¥s the value of data in \\(d\\), in this case, the grad of \\(d\\) is -2 and the grad of \\(f\\) is 4.\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n125139691820544\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n125139691819536*\n\n*\n\n\n\n125139691820544-&gt;125139691819536*\n\n\n\n\n\n125139691821072\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n125139691821792*\n\n*\n\n\n\n125139691821072-&gt;125139691821792*\n\n\n\n\n\n125139691821072+\n\n+\n\n\n\n125139691821072+-&gt;125139691821072\n\n\n\n\n\n125139691819536\n\ne\n\ndata -6.0000\n\ngrad 0.0000\n\n\n\n125139691819536-&gt;125139691821072+\n\n\n\n\n\n125139691819536*-&gt;125139691819536\n\n\n\n\n\n125139691821792\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n125139691821792*-&gt;125139691821792\n\n\n\n\n\n125139691816224\n\nc\n\ndata 10.0000\n\ngrad 0.0000\n\n\n\n125139691816224-&gt;125139691821072+\n\n\n\n\n\n125139691828512\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n125139691828512-&gt;125139691821792*\n\n\n\n\n\n125139691820352\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n125139691820352-&gt;125139691819536*\n\n\n\n\n\n\n\n\n\nNow its start being interesting, we want to calc the gradient of \\(e\\) and \\(c\\) in relation of \\(L\\). From the chain of rule, we have the expression below: \\[\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e}\n\\] Bascially, it¬¥s saying that the gradient of \\(e\\) in relation with \\(L\\) it¬¥s just the gradient of \\(d\\) in relation with \\(L\\) time \\(e\\) in relation with \\(d\\). This mean that we only have to calc the local gradient \\(\\frac{\\partial d}{\\partial e}\\) because we already have calc the \\(\\frac{\\partial L}{\\partial d}\\) and its save in d.grad. The same with \\(c\\) it¬¥s true. So, from calculus, the gradient from an expression like \\(f(x,y) = x + y\\) its 1 for both \\(x\\) and \\(y\\). Thus, we have\n\\[\n\\frac{\\partial d}{\\partial e} = 1\n\\quad \\text{and}  \\quad\n\\frac{\\partial d}{\\partial c} = 1\n\\] \\[\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial e} = -2 \\cdot 1 = -2\n\\quad \\text{and}  \\quad\n\\frac{\\partial L}{\\partial e} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial c} = -2 \\cdot 1 = -2\n\\]\nThis is the strong concept that make the AutoGrad work, we only need to calc the local gradient and multiply with the gradient from the father of nodes (because the gradient of the father already is in relation with the last node).\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n125139691820544\n\na\n\ndata 2.0000\n\ngrad 0.0000\n\n\n\n125139691819536*\n\n*\n\n\n\n125139691820544-&gt;125139691819536*\n\n\n\n\n\n125139691821072\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n125139691821792*\n\n*\n\n\n\n125139691821072-&gt;125139691821792*\n\n\n\n\n\n125139691821072+\n\n+\n\n\n\n125139691821072+-&gt;125139691821072\n\n\n\n\n\n125139691819536\n\ne\n\ndata -6.0000\n\ngrad -2.0000\n\n\n\n125139691819536-&gt;125139691821072+\n\n\n\n\n\n125139691819536*-&gt;125139691819536\n\n\n\n\n\n125139691821792\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n125139691821792*-&gt;125139691821792\n\n\n\n\n\n125139691816224\n\nc\n\ndata 10.0000\n\ngrad -2.0000\n\n\n\n125139691816224-&gt;125139691821072+\n\n\n\n\n\n125139691828512\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n125139691828512-&gt;125139691821792*\n\n\n\n\n\n125139691820352\n\nb\n\ndata -3.0000\n\ngrad 0.0000\n\n\n\n125139691820352-&gt;125139691819536*\n\n\n\n\n\n\n\n\n\nSo for the last two variables \\(a\\) and \\(b\\), its another multiplication, so the local grad of \\(b\\) its data of \\(a\\), and for \\(a\\) its data from \\(b\\). Put in a code, its just\n\nL.grad = 1\nd.grad = f.data\nf.grad = d.data\ne.grad = 1 * d.grad\nc.grad = 1 * d.grad\na.grad = b.data * e.grad\nb.grad = a.data * e.grad\n\n\n\n\n\n    \n\n\n\n\n\n%3\n\n\n\n125139691820544\n\na\n\ndata 2.0000\n\ngrad 6.0000\n\n\n\n125139691819536*\n\n*\n\n\n\n125139691820544-&gt;125139691819536*\n\n\n\n\n\n125139691821072\n\nd\n\ndata 4.0000\n\ngrad -2.0000\n\n\n\n125139691821792*\n\n*\n\n\n\n125139691821072-&gt;125139691821792*\n\n\n\n\n\n125139691821072+\n\n+\n\n\n\n125139691821072+-&gt;125139691821072\n\n\n\n\n\n125139691819536\n\ne\n\ndata -6.0000\n\ngrad -2.0000\n\n\n\n125139691819536-&gt;125139691821072+\n\n\n\n\n\n125139691819536*-&gt;125139691819536\n\n\n\n\n\n125139691821792\n\nL\n\ndata -8.0000\n\ngrad 1.0000\n\n\n\n125139691821792*-&gt;125139691821792\n\n\n\n\n\n125139691816224\n\nc\n\ndata 10.0000\n\ngrad -2.0000\n\n\n\n125139691816224-&gt;125139691821072+\n\n\n\n\n\n125139691828512\n\nf\n\ndata -2.0000\n\ngrad 4.0000\n\n\n\n125139691828512-&gt;125139691821792*\n\n\n\n\n\n125139691820352\n\nb\n\ndata -3.0000\n\ngrad -4.0000\n\n\n\n125139691820352-&gt;125139691819536*\n\n\n\n\n\n\n\n\n\nThis is a complete backward pass manual, know we need to make a code to this automatically for us\n\nAutomatically backward pass\nTo make this automatically, we probably notted that we need to start from the last node and go in a reverse order, this is necessery because for calc the gradient of and node, we need the local gradient of the variable, and the gradient of the father, the unique exception is the last node, because its gradient always will be 1. The way will we implement this, its just make a node do the calc of his childs gradients, lets see the mul function:\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            # We use += over =, because with we use the same node two times it will be reset, and with this node influences in two other nodes, its influence in final result, are the sum of the influence on both nodes\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\nLets check with this can calc the gradients of \\(d\\) and \\(f\\)\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\nL.grad = 1\n\nL._backward()\nprint(\"Grad of d:\", d.grad)\nprint(\"Grad of f:\", f.grad)\n\nGrad of d: -2.0\nGrad of f: 4.0\n\n\nIt works! So know, its just do for add too, below are the complete Value class\n\n\nShow Python code\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n\nLets check with we can calc all the gradients:\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\nL.grad = 1\n\nL._backward() # Node L: Propagates gradient to d and f\nd._backward() # Node d: Propagates gradient to e and c\nf._backward() # Node f (Leaf): Does nothing (empty lambda)\ne._backward() # Node e: Propagates gradient to a and b\nc._backward() # Node c (Leaf): Does nothing\nb._backward() # Node b (Leaf): Does nothing\na._backward() # Node a (Leaf): Does nothing\n\nprint(f\"L data: {L.data}\")\nprint(\"-\" * 20)\nprint(f\"Grad of L: {L.grad}\") # Should be 1\nprint(f\"Grad of f: {f.grad}\") # Should be 4.0 (d.data)\nprint(f\"Grad of d: {d.grad}\") # Should be -2.0 (f.data)\nprint(f\"Grad of c: {c.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of e: {e.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of b: {b.grad}\") # Should be -4.0 (a.data * e.grad -&gt; 2 * -2)\nprint(f\"Grad of a: {a.grad}\") # Should be 6.0 (b.data * e.grad -&gt; -3 * -2)\n\nL data: -8.0\n--------------------\nGrad of L: 1\nGrad of f: 4.0\nGrad of d: -2.0\nGrad of c: -2.0\nGrad of e: -2.0\nGrad of b: -4.0\nGrad of a: 6.0\n\n\nIt worked perfectly, know we only need to make a funcion that calls all _backward() in all nodes recursively. Therefore we will use an algorithm for generating the topological order of the graph. That is, a linear order in which we can execute the nodes without causing any kind of dependency problem. Explaining this algorithm in depth is outside the scope of this project, but it is not that complicated, see below:\n\n# ... All methods of Value class\ndef backward(self):\n    self.grad = 1\n    # Montar ordem topologica\n    topo = []\n    visited = set()\n    def build_topo(v):\n        if v not in visited:\n            visited.add(v)\n            for child in v._prev:\n                build_topo(child)\n            topo.append(v)\n    build_topo(self)\n\n    for node in reversed(topo):\n        node._backward()\n\nKnow, lets test our backward function\n\n\nShow Python code\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None # Add this new parameter too save the func\n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    # This is just for us visualize our class\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        # We just add the data, and return a Value object with new data, and with pointes for the two number that make the out number\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        self.grad = 1\n        # create a topological order\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        for node in reversed(topo):\n            node._backward()\n\n\n\na = Value(2., label=\"a\")\nb = Value(-3.0, label=\"b\")\nc = Value(10., label=\"c\")\nf = Value(-2., label=\"f\")\ne = a * b; e.label=\"e\"\nd = e + c; d.label=\"d\"\nL = d * f; L.label=\"L\"\n\n\nL.backward() \n\n\nprint(f\"L data: {L.data}\")\nprint(\"-\" * 20)\nprint(f\"Grad of L: {L.grad}\") # Should be 1\nprint(f\"Grad of f: {f.grad}\") # Should be 4.0 (d.data)\nprint(f\"Grad of d: {d.grad}\") # Should be -2.0 (f.data)\nprint(f\"Grad of c: {c.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of e: {e.grad}\") # Should be -2.0 (1 * d.grad)\nprint(f\"Grad of b: {b.grad}\") # Should be -4.0 (a.data * e.grad -&gt; 2 * -2)\nprint(f\"Grad of a: {a.grad}\") # Should be 6.0 (b.data * e.grad -&gt; -3 * -2)\n\nL data: -8.0\n--------------------\nGrad of L: 1\nGrad of f: 4.0\nGrad of d: -2.0\nGrad of c: -2.0\nGrad of e: -2.0\nGrad of b: -4.0\nGrad of a: 6.0\n\n\nIt¬¥s worked perfectly again, know we already have a functional AutoGrad system, know, it¬¥s just need to add more operations and we will be capable of create our propely framework of Deep Learning using our own AutoGrad. Before going to the next part, see the code bellow, its the same that we create together, but with some little changes, try to figure out what they‚Äôre for (Hint, they‚Äôre important for the next part)\n\nclass Value:\n    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None \n        self._prev = set(_children) \n        self._op = _op\n        self.label = label \n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out =  Value(self.data * other.data, (self,other), '+')\n        \n        def _backward():\n            self.grad += other.data * out.grad \n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out =  Value(self.data + other.data, (self,other), '+')\n        def _backward():\n            self.grad += out.grad \n            other.grad += out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        self.grad = 1\n        # create a topological order\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        for node in reversed(topo):\n            node._backward()"
  },
  {
    "objectID": "posts/nn-from-scratch-1/index.html#lets-make-our-deep-learning-framework",
    "href": "posts/nn-from-scratch-1/index.html#lets-make-our-deep-learning-framework",
    "title": "Building Neural Networks from Scratch",
    "section": "Lets make our Deep Learning framework",
    "text": "Lets make our Deep Learning framework\n\nBuilding a Neuron\nSo, for now, we have already created our own engine to calculate our gradients. Thus, lets start making a simple artificial neuron. If you don‚Äôt know or don‚Äôt remember how a neuron work, basically it receives inputs, multiply each one by some weight, sum all values, and last, pass this value in a non-linear function\n\n\n\n\n\n\nFigure¬†1: An Artificial Neuron\n\n\n\nTo start, we can make a simple class for our neuron:\n\nimport random\nclass Neuron:\n    def __init__(self, input_num):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(input_num)] # This basicaly creates a list of Value's from a uniform distribuition with lenth equals to input_num\n        self.b = Value(random.uniform(-1,1)) # This is for the bias, explain deeply the importance of this is out of scope, but you can think bias its just a special weight that dosnt multiply the inputs, just sum in the sum part.\n\n    # Call is a special function in python, its like __add__, the sintax for use this is when you just put () after the object, like range(x)\n    def __call__(self,x):\n        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) # This just do the calculation of a neuron until the sum part\n        out = act.tahn() # This calls a non-linear function from the Value class, I will show how to implement bellow\n        return out\n    \n    def paramerters(self):\n        return self.w + [self.b] # this function just return all the parameters of our neuron\n\nAnd, its just it, a completely and functional artificial neuron, or at least after we implement the tahn (tangent hyperbolic) function. It‚Äôs formula is: \\[\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nNote that there is 3 operations that we dont implement to make this, so, lets implement them. For know, I presume that you understand how to implement this, so, try your self, you can search in google for this, just dont use an llm.\n\n\nShow solution:\nimport math\n# Its not really necessery to add exp, subtraction or divided, but its good with you do üòä\ndef tahn(self):\n    x = self.data\n    t = (math.exp(x) - math.exp(-x))/ (math.exp(x) + math.exp(-x))\n    out = Value(t, (self,), \"tahn\")\n\n    # We can derivate, or just pick up on google\n    def _backwards():\n        self.grad = (1 - t**2) * out.grad\n    \n    out._backward = _backward\n    return out\n\n\nKnow, lets test if this are working\n\nx = Value(10)\n\ny = x.tahn()\ny.backward()\ny.data, y.grad, x.data, x.grad\n\n(0.9999999958776926, 1, 10, 8.244614768671e-09)\n\n\nWorks preety well. Let‚Äôs try the neuron\n\nn = Neuron(2)\nx = [2, 3, 4]\nout = n(x)\nprint(out, n.paramerters())\n\nValue(data=-0.9811578018032878) [Value(data=-0.8326666934598519), Value(data=0.07742925925429978), Value(data=-0.8946232717059577)]\n\n\nWorks, lets move on.\n\n\nBuilding an"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Studies Notes",
    "section": "",
    "text": "Building Neural Networks from Scratch\n\n\nPart 1\n\n\n\nscratch\n\n\nen\n\n\ncode\n\n\nmath\n\n\nnn\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nLuca WB\n\n\n\n\n\n\n\n\n\n\n\n\nO B√°sico de Machine Unlearning em LLMs\n\n\n\n\n\n\nunlearning\n\n\npt\n\n\ncode\n\n\nquantization\n\n\nmath\n\n\nllms\n\n\n\n\n\n\n\n\n\nDec 19, 2025\n\n\nLuca WB\n\n\n\n\n\n\nNo matching items"
  }
]